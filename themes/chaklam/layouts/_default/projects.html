{{ define "main" }}

<main id="main">
  <!-- ======= Breadcrumbs ======= -->
  <section class="breadcrumbs">
    <div class="container">
      <h2>{{ .Title }}</h2>
    </div>

    </div>
  </section><!-- End Breadcrumbs -->

  <!-- ======= Portfolio Details Section ======= -->
  <section id="portfolio-details" class="portfolio-details">
    <div class="container">
      {{/*  <p><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen="" data-align="left" frameborder="0" height="315"
          src="https://www.youtube.com/embed/fMaOqt8tdsg?si=RIPSwUZCM2pqOYw6" style="margin-right:30px;"
          title="YouTube video player" width="500"></iframe>
          <iframe width="500" height="315" src="https://www.youtube.com/embed/2e37nmzC6Ig?si=DOcD4HXIalorkoNK" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          <iframe width="500" height="315" src="https://www.youtube.com/embed/DXnu3WY3znc?si=HWOKOSnhvaP1jAiZ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        <iframe width="500" height="315" src="https://www.youtube.com/embed/eK-lxMjvbVU?si=jjQM4k9uuF6kY1Wz" title="YouTube video player" frameborder="0" style="margin-left:30px;" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
      </p>
      <!-- <h3><img alt="fun" data-align="left" data-entity-type="file" data-entity-uuid="2205d992-6a3c-48cc-8185-be3c417db181" height="311" src="/sites/default/files/inline-images/IMG20200917192032.jpg" style="margin-right: 20px;" width="500" /></h3> -->  */}}

      <h3>Culture</h3>

      <p>We strive to build an interdisciplinary team working in the intersetion of <strong>deep learning</strong>, <strong>modeling humans</strong> and <strong>improving overall human's wellbeing</strong>.  Our research area, commonly called, "Human-Centered AI", is similar to that of <a href="https://hai.stanford.edu/" target="_blank">Stanford HAI</a>,   <a href="https://hailab.ox.ac.uk" target="_blank">University of Oxford HAIL</a>, <a href="https://hcii.cmu.edu" target="_blank">Carnegie Mellon HCII</a>, <a href="https://www.media.mit.edu" target="_blank">MIT Media Lab</a>.  We are dedicated to advancing AI in a way that benefits humanity. Our commitment to human-centered AI is evident in our mission to study, guide, and develop AI technologies and applications that are collaborative, augmentative, and enhance human productivity and quality of life.  Our core values are:</p>

      <ul>
        <li><strong>Experimental</strong>: We value scientific rigor, focusing on researching under strong scientific
          grounds and conducting sound experiments that provide definitive and repeatable findings.
        </li>
        <li><strong>Computational</strong>: Our scientific nature is to use algorithms, mathematical models, strong
          theoretical background, and strong coding skills.</li>
      </ul>

      <h3>Lab Entry</h3>

      <p>We welcomed students from all disciplines but those with a huge passion for research.</p>

      <h3>Problems we are interested</h3>

      <p>We are mostly interested in problems in the intersection of deep learning, human psychology, and health and well-being.</p>

      <p>Deep learning</p>
      <ol>
        <li><strong>Hybrid Transformer-SSM Architectures</strong>: How to optimally combine attention with state space components? Can we develop attention-like mechanisms within the SSM framework?What hybrid designs offer best compute/performance tradeoffs? How can interpretability methods guide architecture development?</li>
        <li><strong>Long-Context Modeling Breakthroughs</strong>: How to effectively model 1M+ token contexts? What architectural innovations enable true long-range reasoning? How to evaluate true long-context understanding beyond retrieval?</li>
        <li><strong> Computationally-Aware Scaling Laws</strong>: While scaling laws for transformers have been extensively studied, there's a significant gap in understanding how different architectural families (transformers, SSMs, MoEs) compare in their scaling behavior. Do different architectures (Transformers, SSMs, MoEs) follow the same scaling laws? Can we develop a unified theory of scaling across architectures? Can we predict performance at scale from small-scale experiments?</li>
        <li><strong>Verifiable Multi-Step Reasoning</strong>: How can LLMs be engineered to perform complex, multi-step reasoning processes that are not only accurate but also transparent and verifiable, allowing for an audit trail of their logical derivations? This directly addresses findings that current models may rely on "memorized patterns rather than systematic reasoning"  and builds on interest in "automated reasoning".</li>
      </ol>

      {{/*  <p style="line-height:1.38; margin-top:16px; margin-bottom:16px"><strong>Fundamental Research (suitable for Ph.D. students or Master students who want to go Ph.D.)</strong></p>  */}}
      <p>Modeling humans</p>
      <ol>
        <li><strong>Modeling humans through EEG and physiological signals</strong>: How does EEG and physiological profile looks like when humans are stressed?  engaged?  mindful? etc.  Then can we create an intervention system to improve these properties?</li>
        <li><strong>Modeling humans through language, agents and games</strong>: How can we model beliefs, emotion, cognition, behaviors, culture, and biases, etc. through language, agents, and games? </li>
        <li><strong>Understanding humans through voice and face</strong>: Can we detect depression, confidence, humours, or risk profiles from a person's voice?  What should we ask him/her to speak to detect that?  How long should that voice be?  Does it work for only certain languages? Does it get better results if we combine with facial features?</li>
      </ol>

      <p>Health and wellness</p>
      <ol>
        <li><strong>Medical Imaging</strong>: How to incorporate AI to the current workflow? What data interoperability challenges we need to concern? How to build explaninability and trust? How reasoning can be done?  Can we incorporate VQA?  How does traditional computer vision techniques compared with modern techniques like semi-supervised or self-supervised learning?  How to deal with millions of unlabeled data?  Can we use diffusion or generative AI to create infinite medical images?</li>
        <li><strong>Raman Spectroscopy</strong>: How to measure blood glucose non-invasively using Raman spectroscopy?  Can we put them into a portable low-cost machine and deploy them into households?</li>
        <li><strong>EEG BCI Speller</strong>: How to develop an effective and user-friendly BCI speller for locked-in patients using EEG?</li>
        <li><strong>Chatbot for emotional support</strong>: Can we incorporate CBT/DBT/ACT to chatbot to provide emotional support or assessments to people suffering from stress or depression?</li>
      </ol>

      <p>Internships</p>
      <ol>
        {{/*  <li><strong>Mobile Speech2Text app</strong>:  A mobile-based speech2text for tourism / policy / insurance industry</li>  */}}
        <li><strong>Voice chatbots for emergency </strong>:  Thailand has plan to develop voice chatbot during emergency where it will spams millions of citizens, asking for their well-being. </li>
        <li><strong>Avatar + Voice Cloning + Text2Speech </strong>:  Thailand still do not have its own technology in terms of voice cloning or avatar generation or text2speech, which will be useful in the domain of education and tourism. </li>
        <li><strong>Digital Twins + Truck</strong>: Combining AR/VR, Digital Twin, and AI technologies for trucks enables real-time monitoring, predictive maintenance, and immersive training by creating a virtual replica of the vehicle enhanced with intelligent insights.</li>
        <li><strong>Car Damage Prediction</strong>:  Current car damage prediction still suffers from low accuracy, mostly from outliers not existing in the dataset. </li>
        <li><strong>One Report Generation</strong>:  Using agentic AI to generate whole One report.</li>
        <li><strong>OCR</strong>:  Develop advanced OCR solution that accomodates dynamic forms and handwritings.</li>
        <li><strong>AI Tutor</strong>:  Using avatar technology + agentic AI to reimagine how a AI tutor should look like.</li>
        <li><strong>AI Interviewer</strong>:  Using voice + face + LLM + resume analysis to evaluate and screen candidates.</li>
        {{/*  <li><strong>Template Matching</strong>:  Develop computer vision suite of models to support manufacturing companies.</li>  */}}
      </ol>
    </div>
  </section><!-- End Portfolio Details Section -->
</main>
{{ end }}