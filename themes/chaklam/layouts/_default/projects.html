{{ define "main" }}

<main id="main">
  <section class="breadcrumbs">
    <div class="container">
      <h2>{{ .Title }}</h2>
    </div>
  </section>
  <section id="portfolio-details" class="portfolio-details">
    <div class="container">
      <h3>Culture & Research Philosophy</h3>
      <p>
        Our interdisciplinary team works at the foundational models of
        <strong>computer vision</strong>,
        <strong>natural language processing</strong>, and <strong>multimodal learning</strong>:
      </p>
      <ul>
        <li>
          <strong>Experimental</strong>: Conduct reproducible experiments that
          advance fundamental understanding.
        </li>
        <li>
          <strong>Computational</strong>: Leverage algorithms, models, and
          coding expertise to tackle challenging questions.
        </li>
      </ul>

      <h3>Lab Entry</h3>
      <p>
        We welcome students from all disciplines with strong curiosity and a
        passion for rigorous, original research. We commonly submit our work to
        conferences including ACL, NIPS, CVPR and ACM CHI.
      </p>

      <h3>Topics</h3>
      <p>
        We focus topics around foundatonal deep learning models used in real-world
        applications:
      </p>

      <ol class="project-list">
          <!-- {{/*  <li class="project-item-text">
            <strong>Robust & Fair Biometrics (Vision)</strong><br>
            Standard face recognition works well in perfect conditions, but it often fails when demographics, lighting, or camera angles change. We research how to create "invariant" representations—models that ignore noise and focus on identity. We also work heavily on security, specifically <strong>Liveness Detection</strong> and <strong>Anti-Spoofing</strong>, trying to mathematically distinguish between a real face and a digital attack or mask.
          </li>  */}} -->
          
          <!-- <li class="project-item-text">
            <strong>Document Intelligence</strong><br>
            OCR is no longer just about reading text; it is about understanding the structure of a page. A human understands a document by looking at the layout, charts, and text together. We are building <strong>Multimodal models</strong> that can parse complex, unstructured documents (like invoices or scientific papers) end-to-end. We are especially interested in low-resource languages where training data is scarce.
          </li> -->
          
          <!-- <li class="project-item-text">
            <strong>Small Language Models</strong><br>
            Everyone is competing to build bigger models, but we are going the other direction. We focus on <strong>Small Language Models (SLMs)</strong> and <strong>Knowledge Distillation</strong>. The core research question here is: <em>How much reasoning capability can we retain if we reduce the model size by 10x or 100x?</em> We investigate quantization and parameter-efficient fine-tuning to bring LLM-level intelligence to edge devices.
          </li> -->
          
          <!-- <li class="project-item-text">
            <strong>Speech2Text</strong><br>
            Labelling audio data is expensive and slow. We focus on <strong>Self-Supervised Learning (SSL)</strong>, where models learn the structure of speech just by listening to massive amounts of unlabelled audio. We apply this to difficult problems like "Code-Switching" (when speakers mix languages in one sentence) and speech recognition in highly noisy environments where standard models collapse.
          </li> -->
          
          <!-- <li class="project-item-text">
            <strong>Mixed Speech2Text</strong><br>
            We work on the area of creating models for speech, computer vision, language tasks.  Our focus is on creating models that are more efficient, accurate, and robust and performing rigorous evaluaton.  Our focus is to publish novel solutions for challenging problems in top conferences such as NeurIPS, ICML, ICLR, CVPR, ACL, etc.
          </li> -->


          <li class="project-item-text">
<<<<<<< HEAD
            <strong>Deep learning</strong><br>
            We work on the area of creating models for speech, computer vision, language tasks.  Our focus is on creating models that are more efficient, accurate, and robust and performing rigorous evaluaton.  Our focus is to publish novel solutions for challenging problems in top conferences such as NeurIPS, ICML, ICLR, CVPR, ACL, etc.
=======
            <strong>Facial presentation attacks</strong><br>
            We are researching methods to detect and mitigate the risks associated with facial presentation attacks, particularly the modern ones involving AI-generated synthethic faces which are highly realistic.   Our work includes developing robust algorithms that work under different edge case scenarios, creating presentation attack datasets, understanding quality of training data, and utlimately leading to secure facial recognition systems that will be used in our everday life.
>>>>>>> 29aa4fe (update)
          </li>

          

          <!-- <li class="project-item-text">
            <strong>AI Safety (Deepfakes, voice cloning)</strong><br>
            As generative AI becomes more powerful, ensuring its safe and ethical use is paramount. We are researching methods to detect and mitigate the risks associated with Deepfakes and voice cloning technologies. Our work includes developing robust detection algorithms that can identify manipulated media, as well as exploring strategies to prevent misuse of generative models. We aim to contribute to the broader conversation on AI safety and promote responsible AI development.
          </li> -->
          
          <!-- {{/*  <li class="project-item-text">
            <strong>Multimodal Affective Computing (HCI)</strong><br>
            Human communication is more than just words; it’s tone, pause, and context. We are researching <strong>Context-Aware Emotion Recognition</strong>. Instead of basic "happy/sad" classification, we try to model complex internal states like cognitive load or stress by fusing audio and text data. We are particularly interested in how these models generalize across different cultures and languages.
          </li>  */}} -->
          <li class="project-item-text">
            <strong>Human-AI Interaction with Mental Health and Human Cultures and Biases</strong><br>
            Our lab has been working on understanding humans, particularly in the area of biases, culture, cognition, emotion, behavior.  Through that understanding, we are interested in how to design AI systems that are more intuitive and user-friendly and can effectively collaborate with humans for stronger outcomes.  Example applications that we have been working for a long tiem include mental health related applications and tutoring related applications.    
          </li>
          <li class="project-item-text">
            <strong>Mixed speech recognition</strong><br>
            We are researching methods to improve the performance of speech recognition systems in challenging scenarios, such as when speakers mix languages in one sentence (code-switching) or when there is a lot of background noise.   Our work includes developing robust algorithms that can handle these edge cases, creating datasets for training and evaluation, and ultimately leading to more accurate and reliable speech recognition systems that can be used in real-world applications.
          </li>

          <!-- <li class="project-item-text">
            <strong>AI Safety (Deepfakes, voice cloning)</strong><br>
            As generative AI becomes more powerful, ensuring its safe and ethical use is paramount. We are researching methods to detect and mitigate the risks associated with Deepfakes and voice cloning technologies. Our work includes developing robust detection algorithms that can identify manipulated media, as well as exploring strategies to prevent misuse of generative models. We aim to contribute to the broader conversation on AI safety and promote responsible AI development.
          </li> -->
          
          <!-- {{/*  <li class="project-item-text">
            <strong>Multimodal Affective Computing (HCI)</strong><br>
            Human communication is more than just words; it’s tone, pause, and context. We are researching <strong>Context-Aware Emotion Recognition</strong>. Instead of basic "happy/sad" classification, we try to model complex internal states like cognitive load or stress by fusing audio and text data. We are particularly interested in how these models generalize across different cultures and languages.
          </li>  */}} -->
          <li class="project-item-text">
            <strong>Human-AI Interaction with Mental Health and Human Cultures and Biases</strong><br>
            Our lab has been working on understanding humans, particularly in the area of biases, culture, cognition, emotion, behavior.  Through that understanding, we are interested in how to design AI systems that are more intuitive and user-friendly and can effectively collaborate with humans for stronger outcomes.  Example applications that we have been working for a long tiem include mental health related applications and tutoring related applications.    
          </li>
          <li class="project-item-text">
            <strong>Medical VQA</strong><br>
            Visual Question Answering (VQA) in the medical domain is a challenging task that requires a deep understanding of both visual data and medical knowledge. We are developing advanced VQA systems that can assist healthcare professionals by providing accurate answers to complex medical questions based on medical images such as X-rays, MRIs, and CT scans. Our research focuses on integrating multimodal data and leveraging domain-specific knowledge to improve the accuracy and reliability of these systems.
          </li>
          <li class="project-item-text">
            <strong>BCI spellers</strong><br>
            Our lab has been developing BCI spellers for almost a decade.  We worked with both SSVEP paragadigm, P300 paradigm, and hybrid paradigms.   We have performed many iterative improvements on the system, including optimizing the visual stimuli, improving the signal processing and classification algorithms, and enhancing the user interface.   The ongoing research focus is to improve the information transfer rate (ITR) and usability of the BCI spellers with new technologies such as deep learning and transfer learning.
          </li>
          <li class="project-item-text">
            <strong>Raman spectroscopy for non-invasive glucose monitoring</strong><br>
            Diabetes is a major health issue worldwide, and non-invasive glucose monitoring has the potential to greatly improve the quality of life for diabetic patients. Our lab has been working on developing a non-invasive glucose monitoring system using Raman spectroscopy. Our goal is to design and develop a portable Raman system that can help elderly and patients for glucose monitoring.

          </li>
      </ol>
    </div>
  </section>
</main>

<style>
  .project-list {
    /* CHANGED: set to decimal to show numbers 1., 2., 3. */
    list-style: bullet; 
    /* CHANGED: Added padding so the numbers aren't hidden off-screen */
    padding-left: 20px; 
    margin-left: 20px;
  }
  .project-item-text {
    margin-bottom: 5px;
    padding-left: 0px; /* Adds a little space between the number and the text */
  }
  
  /* Retaining your original styles for responsiveness just in case */
  @media (max-width: 768px) {
    .project-item {
      flex-direction: column;
      align-items: center;
      text-align: center;
    }
  }
</style>

{{ end }}