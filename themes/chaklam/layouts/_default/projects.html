{{ define "main" }}

<main id="main">
  <section class="breadcrumbs">
    <div class="container">
      <h2>{{ .Title }}</h2>
    </div>
  </section>
  <section id="portfolio-details" class="portfolio-details">
    <div class="container">
      <h3>Culture & Research Philosophy</h3>
      <p>
        Our interdisciplinary team works at the foundational models of
        <strong>computer vision</strong>,
        <strong>natural language processing</strong>, and <strong>multimodal learning</strong>:
      </p>
      <ul>
        <li>
          <strong>Experimental</strong>: Conduct reproducible experiments that
          advance fundamental understanding.
        </li>
        <li>
          <strong>Computational</strong>: Leverage algorithms, models, and
          coding expertise to tackle challenging questions.
        </li>
      </ul>

      <h3>Lab Entry</h3>
      <p>
        We welcome students from all disciplines with strong curiosity and a
        passion for rigorous, original research. We commonly submit our work to
        conferences including ACL, NIPS, CVPR and ACM CHI.
      </p>

      <ol class="project-list">
          {{/*  <li class="project-item-text">
            <strong>Robust & Fair Biometrics (Vision)</strong><br>
            Standard face recognition works well in perfect conditions, but it often fails when demographics, lighting, or camera angles change. We research how to create "invariant" representations—models that ignore noise and focus on identity. We also work heavily on security, specifically <strong>Liveness Detection</strong> and <strong>Anti-Spoofing</strong>, trying to mathematically distinguish between a real face and a digital attack or mask.
          </li>  */}}
          
          <li class="project-item-text">
            <strong>Document Intelligence</strong><br>
            OCR is no longer just about reading text; it is about understanding the structure of a page. A human understands a document by looking at the layout, charts, and text together. We are building <strong>Multimodal models</strong> that can parse complex, unstructured documents (like invoices or scientific papers) end-to-end. We are especially interested in low-resource languages where training data is scarce.
          </li>
          
          <li class="project-item-text">
            <strong>Small Language Models</strong><br>
            Everyone is competing to build bigger models, but we are going the other direction. We focus on <strong>Small Language Models (SLMs)</strong> and <strong>Knowledge Distillation</strong>. The core research question here is: <em>How much reasoning capability can we retain if we reduce the model size by 10x or 100x?</em> We investigate quantization and parameter-efficient fine-tuning to bring LLM-level intelligence to edge devices.
          </li>
          
          <li class="project-item-text">
            <strong>Speech2Text</strong><br>
            Labelling audio data is expensive and slow. We focus on <strong>Self-Supervised Learning (SSL)</strong>, where models learn the structure of speech just by listening to massive amounts of unlabelled audio. We apply this to difficult problems like "Code-Switching" (when speakers mix languages in one sentence) and speech recognition in highly noisy environments where standard models collapse.
          </li>
          
          <li class="project-item-text">
            <strong>Text2Speech</strong><br>
            We work on the dual edges of generative audio. On the creative side, we are building <strong>Controllable TTS</strong> systems that allow fine-grained control over emotion, speed, and prosody without needing hours of studio recording. On the safety side, we are developing forensic tools to detect Deepfakes. We want to find the subtle statistical artifacts that generative models leave behind, so we can distinguish AI voices from human ones.
          </li>

          <li class="project-item-text">
            <strong>Medical VQA</strong><br>
            Visual Question Answering (VQA) in the medical domain is a challenging task that requires a deep understanding of both visual data and medical knowledge. We are developing advanced VQA systems that can assist healthcare professionals by providing accurate answers to complex medical questions based on medical images such as X-rays, MRIs, and CT scans. Our research focuses on integrating multimodal data and leveraging domain-specific knowledge to improve the accuracy and reliability of these systems.
          </li>
          
          
          {{/*  <li class="project-item-text">
            <strong>Multimodal Affective Computing (HCI)</strong><br>
            Human communication is more than just words; it’s tone, pause, and context. We are researching <strong>Context-Aware Emotion Recognition</strong>. Instead of basic "happy/sad" classification, we try to model complex internal states like cognitive load or stress by fusing audio and text data. We are particularly interested in how these models generalize across different cultures and languages.
          </li>  */}}
          <li class="project-item-text">
            <strong>Human-AI Interaction</strong><br>
            As AI systems become more prevalent, understanding how humans interact with them is crucial. We study <strong>Human-AI Interaction</strong> to design systems that are intuitive and user-friendly. Our research focuses on creating interfaces that facilitate seamless collaboration between humans and AI, ensuring that the technology enhances user experience without overwhelming them.  Example applications include AI assistants, Mental Health Chatbots,  recommendation systems, and interactive learning platforms.
          </li>
      </ol>
    </div>
  </section>
</main>

<style>
  .project-list {
    /* CHANGED: set to decimal to show numbers 1., 2., 3. */
    list-style: bullet; 
    /* CHANGED: Added padding so the numbers aren't hidden off-screen */
    padding-left: 20px; 
    margin-left: 20px;
  }
  .project-item-text {
    margin-bottom: 5px;
    padding-left: 0px; /* Adds a little space between the number and the text */
  }
  
  /* Retaining your original styles for responsiveness just in case */
  @media (max-width: 768px) {
    .project-item {
      flex-direction: column;
      align-items: center;
      text-align: center;
    }
  }
</style>

{{ end }}